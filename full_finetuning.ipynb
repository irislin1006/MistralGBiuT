{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.41s/it]\n"
     ]
    }
   ],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config) # .to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512, padding_side=\"left\", add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # Convert the model to 16-bit precision\n",
    "# if torch.cuda.is_available():\n",
    "#     original_model = original_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 262410240\n",
      "all model parameters: 3752071168\n",
      "percentage of trainable model parameters: 6.99%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Unnatural Instruction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 66010\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"mrm8488/unnatural-instructions-core\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 52808\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 6601\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 6601\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into train, test, and validation sets\n",
    "train_test_dataset = dataset['train'].train_test_split(test_size=0.2, seed=42)  # 80% train, 20% for test and validation\n",
    "test_val_dataset = train_test_dataset['test'].train_test_split(test_size=0.5, seed=42)  # Split the 20% equally into test and validation\n",
    "\n",
    "# Create a new DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_test_dataset['train'],\n",
    "    'test': test_val_dataset['train'],\n",
    "    'validation': test_val_dataset['test']\n",
    "})\n",
    "\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instructions: Given a piece of text and a list of keywords, you need to figure out if the text contains all the given keywords at least once or not. If it does contain all of them, output 'True'. Otherwise, output 'False'. Note that keyword case is not important here i.e. whether the keyword is in upper case or lower case does not matter.\n",
      "instances: [{'instruction_with_input': \"Given a piece of text and a list of keywords, you need to figure out if the text contains all the given keywords at least once or not. If it does contain all of them, output 'True'. Otherwise, output 'False'. Note that keyword case is not important here i.e. whether the keyword is in upper case or lower case does not matter.\\nText: The quick brown fox jumps over the lazy dog.,Keywords: ['fox', 'dog', 'brown'].\", 'input': \"Text: The quick brown fox jumps over the lazy dog.,Keywords: ['fox', 'dog', 'brown'].\", 'constraints': \"The output should be one of two strings - either 'True' or 'False'.\", 'output': 'True'}]\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "print(f\"instructions: {dataset_dict['test'][index]['instruction']}\")\n",
    "print(f\"instances: {dataset_dict['test'][index]['instances']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': \"Given a piece of text and a list of keywords, you need to figure out if the text contains all the given keywords at least once or not. If it does contain all of them, output 'True'. Otherwise, output 'False'. Note that keyword case is not important here i.e. whether the keyword is in upper case or lower case does not matter.\", 'instances': [{'instruction_with_input': \"Given a piece of text and a list of keywords, you need to figure out if the text contains all the given keywords at least once or not. If it does contain all of them, output 'True'. Otherwise, output 'False'. Note that keyword case is not important here i.e. whether the keyword is in upper case or lower case does not matter.\\nText: The quick brown fox jumps over the lazy dog.,Keywords: ['fox', 'dog', 'brown'].\", 'input': \"Text: The quick brown fox jumps over the lazy dog.,Keywords: ['fox', 'dog', 'brown'].\", 'constraints': \"The output should be one of two strings - either 'True' or 'False'.\", 'output': 'True'}]}\n",
      "[{'instruction_with_input': \"Given a piece of text and a list of keywords, you need to figure out if the text contains all the given keywords at least once or not. If it does contain all of them, output 'True'. Otherwise, output 'False'. Note that keyword case is not important here i.e. whether the keyword is in upper case or lower case does not matter.\\nText: The quick brown fox jumps over the lazy dog.,Keywords: ['fox', 'dog', 'brown'].\", 'input': \"Text: The quick brown fox jumps over the lazy dog.,Keywords: ['fox', 'dog', 'brown'].\", 'constraints': \"The output should be one of two strings - either 'True' or 'False'.\", 'output': 'True'}] <class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"<s>[INST] instruction_with_input: Given a piece of text and a list of keywords, you need to figure out if the text contains all the given keywords at least once or not. If it does contain all of them, output 'True'. Otherwise, output 'False'. Note that keyword case is not important here i.e. whether the keyword is in upper case or lower case does not matter.\\nText: The quick brown fox jumps over the lazy dog.,Keywords: ['fox', 'dog', 'brown'].,\\nconstraints: The output should be one of two strings - either 'True' or 'False'.input: Text: The quick brown fox jumps over the lazy dog.,Keywords: ['fox', 'dog', 'brown'].,\\ninstruction_with_input: Given a piece of text and a list of keywords, you need to figure out if the text contains all the given keywords at least once or not. If it does contain all of them, output 'True'. Otherwise, output 'False'. Note that keyword case is not important here i.e. whether the keyword is in upper case or lower case does not matter.\\nText: The quick brown fox jumps over the lazy dog.,Keywords: ['fox', 'dog', 'brown'].,\\nconstraints: The output should be one of two strings - either 'True' or 'False'. [/INST]\",\n",
       " 'True')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def get_prompt_output(row):\n",
    "#     # print(row)\n",
    "#     inst_dict_list = row['instances']\n",
    "#     print(inst_dict_list, type(inst_dict_list))\n",
    "#     # Ignore input for now \"input: {inst_dict['input']},\\n\"\n",
    "#     input_text_list = [f\"instruction_with_input: {inst_dict['instruction_with_input']},\\nconstraints: {inst_dict['constraints']}\"f\"input: {inst_dict['input']},\\ninstruction_with_input: {inst_dict['instruction_with_input']},\\nconstraints: {inst_dict['constraints']}\"\n",
    "#                   for inst_dict in inst_dict_list]\n",
    "#     input_text = \"\\n\".join(input_text_list)\n",
    "#     prompt = f\"<s>[INST] {input_text} [/INST]\"\n",
    "\n",
    "    \n",
    "#     output_list = [inst_dict['output'] for inst_dict in inst_dict_list]\n",
    "#     output = \"\\n\".join(output_list)\n",
    "\n",
    "#     return prompt, output\n",
    "\n",
    "# get_prompt_output(dataset_dict['test'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 52808/52808 [00:12<00:00, 4317.79 examples/s]\n",
      "Map: 100%|██████████| 6601/6601 [00:01<00:00, 4029.08 examples/s]\n",
      "Map: 100%|██████████| 6601/6601 [00:01<00:00, 4244.81 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 52808\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 6601\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 6601\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prompt(d_li):\n",
    "    p_li  = []\n",
    "    for d in d_li:\n",
    "        instruction_with_input = f\"instruction_with_input: {d['instruction_with_input']}\"\n",
    "        constraints = f\"constraints: {d['constraints']}\"\n",
    "        p_li.append(f\"<s>[INST] {instruction_with_input},\\n{constraints} [/INST]\")\n",
    "    return \"\\n\".join(p_li)\n",
    "    # instruction_with_input = f\"instruction_with_input: {d['instruction_with_input']}\"\n",
    "    # constraints = f\"constraints: {d['constraints']}\"\n",
    "    # return f\"<s>[INST] {instruction_with_input},\\n{constraints} [/INST]\"\n",
    "\n",
    "def get_output(d_li):\n",
    "    return \"\\n\".join([d['output'] for d in d_li])\n",
    "\n",
    "def tokenize_function(example):\n",
    "    prompts = [get_prompt(per_inst_d_li) for per_inst_d_li in example['instances']]\n",
    "    outputs = [get_output(per_inst_d_li) for per_inst_d_li in example['instances']]\n",
    "    # raw_p, out = get_prompt_output(example)\n",
    "    # prompt, output = raw_p, out\n",
    "    example['input_ids'] = tokenizer(prompts, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids # .to(device)\n",
    "    example['labels'] = tokenizer(outputs, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    return example\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True, remove_columns=['instruction', 'instances']) # \n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To save some time in the lab, you will subsample the dataset:\n",
    "# tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (52808, 2)\n",
      "Validation: (6601, 2)\n",
      "Test: (6601, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 52808\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 6601\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 6601\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_datasets['train'][4]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 733, 16289, 28793, 13126, 28730, 3415, 28730, 2537, 28747, 995, 460, 2078, 989, 3569, 297, 264, 17237, 8573, 28723, 3604, 3638, 349, 298, 1300, 272, 21638, 384, 4632, 1444, 1167, 989, 3569, 28723, 13, 28732, 28740, 28725, 28705, 28740, 28731, 304, 325, 28734, 28725, 28705, 28734, 609, 28725, 13, 514, 17005, 28747, 415, 3825, 1023, 347, 396, 11584, 6517, 821, 442, 6530, 298, 28705, 28734, 28723, 733, 28748, 16289, 28793, 2]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 28705, 28740, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train'][0]['input_ids'])\n",
    "print(tokenized_datasets['train'][0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fin-Tune the Model with Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./model/dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train() ### OOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 3752071168\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "original_model.enable_input_require_grads()\n",
    "original_model.gradient_checkpointing_enable()\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type='CAUSAL_LM'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 21260288\n",
      "all model parameters: 3773331456\n",
      "percentage of trainable model parameters: 0.56%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./peft-causal-lm-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1    \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mirislin1006\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/iris/Desktop/wsl_shared/mistralAi/wandb/run-20240123_102439-dzve64i5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/irislin1006/huggingface/runs/dzve64i5' target=\"_blank\">distinctive-forest-6</a></strong> to <a href='https://wandb.ai/irislin1006/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/irislin1006/huggingface' target=\"_blank\">https://wandb.ai/irislin1006/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/irislin1006/huggingface/runs/dzve64i5' target=\"_blank\">https://wandb.ai/irislin1006/huggingface/runs/dzve64i5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.868700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=12.868653297424316, metrics={'train_runtime': 14.4526, 'train_samples_per_second': 0.554, 'train_steps_per_second': 0.069, 'total_flos': 175274075357184.0, 'train_loss': 12.868653297424316, 'epoch': 0.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./peft-causal-lm-checkpoint-local/tokenizer_config.json',\n",
       " './peft-causal-lm-checkpoint-local/special_tokens_map.json',\n",
       " './peft-causal-lm-checkpoint-local/tokenizer.model',\n",
       " './peft-causal-lm-checkpoint-local/added_tokens.json',\n",
       " './peft-causal-lm-checkpoint-local/tokenizer.json')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s>[INST] instruction_with_input: You are given a review and your task is to determine its sentiment. A positive review has a positive sentiment while a negative review has a negative sentiment.\\nThe plot was convoluted and hard to follow.,\\nconstraints: The output should be one of the two strings 'positive' or 'negative'. [/INST]\",\n",
       " \"<s>[INST] instruction_with_input: You are given a date. Your task is to determine whether the date follows the specified format or not.\\n12/05/2015.,\\nconstraints: The correct format for this particular date would be DD/MM/YYYY, where DD represents day, MM month and YYYY year. Therefore, 12/05/2015 satisfies this condition and hence the output should be 'True'. [/INST]\"]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = dataset_dict['test'][0:2]['instances']\n",
    "[get_prompt(per_inst_d_li) for per_inst_d_li in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# ft_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 ['negative', 'True']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[INST] instruction_with_input: You are given a...</td>\n",
       "      <td>[INST] instruction_with_input: You are given a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST] instruction_with_input: You are given a...</td>\n",
       "      <td>[INST] instruction_with_input: You are given a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[INST] instruction_with_input: In this task, y...</td>\n",
       "      <td>[INST] instruction_with_input: In this task, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[INST] instruction_with_input: In this task, y...</td>\n",
       "      <td>[INST] instruction_with_input: In this task, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[INST] instruction_with_input: You are given a...</td>\n",
       "      <td>[INST] instruction_with_input: You are given a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[INST] instruction_with_input: You will be giv...</td>\n",
       "      <td>[INST] instruction_with_input: You will be giv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[INST] instruction_with_input: You are given a...</td>\n",
       "      <td>[INST] instruction_with_input: You are given a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[INST] instruction_with_input: Given two words...</td>\n",
       "      <td>[INST] instruction_with_input: Given two words...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[INST] instruction_with_input: In this task, y...</td>\n",
       "      <td>[INST] instruction_with_input: In this task, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[INST] instruction_with_input: Given a descrip...</td>\n",
       "      <td>[INST] instruction_with_input: Given a descrip...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            original_model_summaries  \\\n",
       "0  [INST] instruction_with_input: You are given a...   \n",
       "1  [INST] instruction_with_input: You are given a...   \n",
       "2  [INST] instruction_with_input: In this task, y...   \n",
       "3  [INST] instruction_with_input: In this task, y...   \n",
       "4  [INST] instruction_with_input: You are given a...   \n",
       "5  [INST] instruction_with_input: You will be giv...   \n",
       "6  [INST] instruction_with_input: You are given a...   \n",
       "7  [INST] instruction_with_input: Given two words...   \n",
       "8  [INST] instruction_with_input: In this task, y...   \n",
       "9  [INST] instruction_with_input: Given a descrip...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  [INST] instruction_with_input: You are given a...  \n",
       "1  [INST] instruction_with_input: You are given a...  \n",
       "2  [INST] instruction_with_input: In this task, y...  \n",
       "3  [INST] instruction_with_input: In this task, y...  \n",
       "4  [INST] instruction_with_input: You are given a...  \n",
       "5  [INST] instruction_with_input: You will be giv...  \n",
       "6  [INST] instruction_with_input: You are given a...  \n",
       "7  [INST] instruction_with_input: Given two words...  \n",
       "8  [INST] instruction_with_input: In this task, y...  \n",
       "9  [INST] instruction_with_input: Given a descrip...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_instances = dataset_dict['test'][0:10]['instances']\n",
    "prompts = [get_prompt(per_inst_d_li) for per_inst_d_li in eval_instances]\n",
    "outputs = [get_output(per_inst_d_li) for per_inst_d_li in eval_instances]\n",
    "#print(prompts)\n",
    "print(len(prompts), len(outputs), outputs[0:2])\n",
    "\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "#     prompt = f\"\"\"\n",
    "# Summarize the following conversation.\n",
    "\n",
    "# {dialogue}\n",
    "\n",
    "# Summary: \"\"\"\n",
    "    \n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    original_model_outputs = original_model.generate(**model_input, max_new_tokens=100)\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(**model_input, max_new_tokens=100)\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(original_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['original_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] instruction_with_input: You are given a review and your task is to determine its sentiment. A positive review has a positive sentiment while a negative review has a negative sentiment.\n",
      "The plot was convoluted and hard to follow.,\n",
      "constraints: The output should be one of the two strings 'positive' or 'negative'. [/INST]\n",
      "\n",
      "[SOL]\n",
      "\n",
      "def sentiment(review):\n",
      "    if review.lower().count('positive') > review.lower().count('negative'):\n",
      "        return 'positive'\n",
      "    else:\n",
      "        return 'negative'\n",
      "\n",
      "[/SOL]\n"
     ]
    }
   ],
   "source": [
    "print(df['original_model_summaries'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] instruction_with_input: You are given a review and your task is to determine its sentiment. A positive review has a positive sentiment while a negative review has a negative sentiment.\n",
      "The plot was convoluted and hard to follow.,\n",
      "constraints: The output should be one of the two strings 'positive' or 'negative'. [/INST]\n",
      "\n",
      "[SOL]\n",
      "\n",
      "def sentiment(review):\n",
      "    if review.lower().count('positive') > review.lower().count('negative'):\n",
      "        return 'positive'\n",
      "    else:\n",
      "        return 'negative'\n",
      "\n",
      "[/SOL]\n"
     ]
    }
   ],
   "source": [
    "print(df['peft_model_summaries'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral_ai_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
