{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Fine-Tune Mistral 7B using LoRA\n",
    "We use [unnatural-instructions](https://github.com/orhonovich/unnatural-instructions) to perform instruction fune-tuning on mistral 7B base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.14s/it]\n"
     ]
    }
   ],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config) # .to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512, padding_side=\"left\", add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print number of trainable model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 262410240\n",
      "all model parameters: 3752071168\n",
      "percentage of trainable model parameters: 6.99%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Unnatural Instruction Dataset through Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 66010\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"mrm8488/unnatural-instructions-core\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode 'instances' into separate rows and make sure exploded_dataset remains the same schema as the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 68478\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_datasets = {}\n",
    "\n",
    "for split, ds in dataset.items():\n",
    "    # Prepare lists to hold the exploded rows\n",
    "    exploded_instructions, exploded_instances = [], []\n",
    "\n",
    "    # Iterate over each row in the dataset\n",
    "    for row in ds:\n",
    "        instruction = row['instruction']\n",
    "        for instance in row['instances']:\n",
    "            # For each instance, create a new row with the same instruction\n",
    "            exploded_instructions.append(instruction)\n",
    "            exploded_instances.append([instance])\n",
    "    \n",
    "    # Create a new dataset from the exploded rows\n",
    "    exploded_data = {'instruction': exploded_instructions, 'instances': exploded_instances}\n",
    "    new_datasets[split] = Dataset.from_dict(exploded_data)\n",
    "\n",
    "# Combine the new datasets into a DatasetDict\n",
    "exploded_dataset = DatasetDict(new_datasets)\n",
    "\n",
    "exploded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'You will be given a series of words. Output these words in reverse order, with each word on its own line.',\n",
       " 'instances': [{'constraints': 'None.',\n",
       "   'input': \"Words: ['Hello', 'world'].\",\n",
       "   'instruction_with_input': \"You will be given a series of words. Output these words in reverse order, with each word on its own line.\\nWords: ['Hello', 'world'].\",\n",
       "   'output': 'world\\nHello'}]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check each instance format\n",
    "exploded_dataset['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to make sure that each row has only one instance\n",
    "sanity_df = exploded_dataset['train'].to_pandas()\n",
    "\n",
    "sanity_df['instances_length'] = sanity_df['instances'].apply(len)\n",
    "assert sanity_df[sanity_df['instances_length'] != 1].empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train, val, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 54782\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 6848\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 6848\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into train, test, and validation sets\n",
    "train_test_dataset = exploded_dataset['train'].train_test_split(test_size=0.2, seed=42)  # 80% train, 20% for test and validation\n",
    "test_val_dataset = train_test_dataset['test'].train_test_split(test_size=0.5, seed=42)  # Split the 20% equally into test and validation\n",
    "\n",
    "# Create a new DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_test_dataset['train'],\n",
    "    'test': test_val_dataset['train'],\n",
    "    'validation': test_val_dataset['test']\n",
    "})\n",
    "\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instructions: You are given a passage with certain words/phrases bolded. Identify if the word/phrase is being used in context ofSTAGE DIRECTIONS orCHARACTERIZATION.\n",
      "instances: [{'constraints': 'The output for each word should be either 0 (Stage Direction) or 1 (Characterization).', 'input': 'STAGE DIRECTIONS:As he enters,MACBETH sees the three witches.He stopshis steps CHARACTERIZATION: amazed and terrified at their sight.', 'instruction_with_input': 'You are given a passage with certain words/phrases bolded. Identify if the word/phrase is being used in context ofSTAGE DIRECTIONS orCHARACTERIZATION.\\nSTAGE DIRECTIONS:As he enters,MACBETH sees the three witches.He stopshis steps CHARACTERIZATION: amazed and terrified at their sight.', 'output': '0\\n1\\n0\\n1'}]\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "print(f\"instructions: {dataset_dict['test'][index]['instruction']}\")\n",
    "print(f\"instances: {dataset_dict['test'][index]['instances']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prompt_output(row):\n",
    "#     # print(row)\n",
    "#     inst_dict_list = row['instances']\n",
    "#     print(inst_dict_list, type(inst_dict_list))\n",
    "#     # Ignore input for now \"input: {inst_dict['input']},\\n\"\n",
    "#     input_text_list = [f\"instruction_with_input: {inst_dict['instruction_with_input']},\\nconstraints: {inst_dict['constraints']}\"f\"input: {inst_dict['input']},\\ninstruction_with_input: {inst_dict['instruction_with_input']},\\nconstraints: {inst_dict['constraints']}\"\n",
    "#                   for inst_dict in inst_dict_list]\n",
    "#     input_text = \"\\n\".join(input_text_list)\n",
    "#     prompt = f\"<s>[INST] {input_text} [/INST]\"\n",
    "\n",
    "    \n",
    "#     output_list = [inst_dict['output'] for inst_dict in inst_dict_list]\n",
    "#     output = \"\\n\".join(output_list)\n",
    "\n",
    "#     return prompt, output\n",
    "\n",
    "# get_prompt_output(dataset_dict['test'][index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Dataset to Generate Prompt|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 54782/54782 [00:12<00:00, 4281.77 examples/s]\n",
      "Map: 100%|██████████| 6848/6848 [00:01<00:00, 4338.50 examples/s]\n",
      "Map: 100%|██████████| 6848/6848 [00:01<00:00, 4508.05 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 54782\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 6848\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 6848\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prompt(d_li):\n",
    "    p_li  = []\n",
    "    for d in d_li:\n",
    "        constraints = f\"\\n{d['constraints']}\" if d['constraints'] else \"\"\n",
    "        instruction = f\"{d['instruction_with_input']}{constraints}\"\n",
    "        model_answer = d['output']\n",
    "\n",
    "        prompt = f\"<s>[INST] {instruction} [/INST] {model_answer}</s>\"\n",
    "        p_li.append(prompt)\n",
    "    return \"\\n\".join(p_li)\n",
    "    # instruction_with_input = f\"instruction_with_input: {d['instruction_with_input']}\"\n",
    "    # constraints = f\"constraints: {d['constraints']}\"\n",
    "    # return f\"<s>[INST] {instruction_with_input},\\n{constraints} [/INST]\"\n",
    "\n",
    "def get_output(d_li):\n",
    "    return \"\\n\".join([d['output'] for d in d_li])\n",
    "\n",
    "def tokenize_function(example):\n",
    "    prompts = [get_prompt(per_inst_d_li) for per_inst_d_li in example['instances']]\n",
    "    outputs = [get_output(per_inst_d_li) for per_inst_d_li in example['instances']]\n",
    "    # raw_p, out = get_prompt_output(example)\n",
    "    # prompt, output = raw_p, out\n",
    "    example['input_ids'] = tokenizer(prompts, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids # .to(device)\n",
    "    example['labels'] = tokenizer(outputs, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    return example\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True, remove_columns=['instruction', 'instances']) # \n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 54782/54782 [00:10<00:00, 5308.74 examples/s]\n",
      "Filter: 100%|██████████| 6848/6848 [00:01<00:00, 5237.39 examples/s]\n",
      "Filter: 100%|██████████| 6848/6848 [00:01<00:00, 5192.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# To save some time in the lab, you will subsample the dataset:\n",
    "subsampled_tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "\n",
    "subsampled_tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (54782, 2)\n",
      "Validation: (6848, 2)\n",
      "Test: (6848, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 54782\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 6848\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 6848\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input len is 512\n",
      "input Ids: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 733, 16289, 28793, 995, 622, 347, 2078, 264, 5511, 302, 2245, 2477, 684, 396, 15048, 1951, 28725, 442, 264, 2952, 6251, 28723, 1047, 272, 1951, 3969, 8313, 354, 368, 28725, 442, 272, 2952, 6251, 2368, 28742, 28707, 1038, 3367, 298, 2918, 574, 901, 1053, 1058, 28725, 3825, 464, 2012, 7285, 647, 5860, 3825, 464, 2249, 1899, 1070, 4135, 13, 1874, 28747, 415, 5252, 5970, 302, 264, 1712, 28733, 18387, 3667, 304, 1539, 28742, 28707, 1721, 778, 7769, 28723, 13, 1014, 3825, 1023, 347, 624, 302, 272, 989, 28747, 464, 2012, 7285, 28742, 442, 464, 2249, 1899, 1070, 4135, 733, 28748, 16289, 28793, 2236, 7285, 2, 2]\n",
      "labels: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2236, 7285, 2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"input len is {len(tokenized_datasets['train'][4]['input_ids'])}\")\n",
    "print(f\"input Ids: {tokenized_datasets['train'][0]['input_ids']}\")\n",
    "print(f\"labels: {tokenized_datasets['train'][0]['labels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fin-Tune the Model with Preprocessed Dataset\n",
    "(Warning) It will OOM on my machine, so I marked them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = f'./model/dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     learning_rate=1e-5,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     num_train_epochs=1,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_steps=1,\n",
    "#     max_steps=1,\n",
    "#     fp16=True,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=original_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets['train'],\n",
    "#     eval_dataset=tokenized_datasets['validation'],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train() ### OOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 3752071168\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "original_model.enable_input_require_grads()\n",
    "original_model.gradient_checkpointing_enable()\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type='CAUSAL_LM'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 21260288\n",
      "all model parameters: 3773331456\n",
      "percentage of trainable model parameters: 0.56%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./peft-causal-lm-training-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    max_steps=1    \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.883500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=12.883462905883789, metrics={'train_runtime': 11.1972, 'train_samples_per_second': 0.714, 'train_steps_per_second': 0.089, 'total_flos': 175274075357184.0, 'train_loss': 12.883462905883789, 'epoch': 0.0})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
       " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
       " './peft-dialogue-summary-checkpoint-local/tokenizer.model',\n",
       " './peft-dialogue-summary-checkpoint-local/added_tokens.json',\n",
       " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>[INST] You are given a list of movie ratings. Each rating consists of a movie title and the corresponding rating out of 5 stars. The task is to find all movies with a rating greater than or equal to 4 stars and print them in alphabetical order.\\nAnt-Man - 4stars Captain America: Civil War - 5stars Deadpool - 3stars Guardians of the Galaxy Vol. 2 - 5stars Logan - 4stars Spider-Man: Homecoming - 5stars Thor Ragnarok - 4stars.\\nPrint each movie title on a new line in alphabetical order. [/INST] Ant-Man\\nCaptain America: Civil War\\nGuardians of the Galaxy Vol. 2\\nLogan\\nSpider-Man: Homecoming\\nThor Ragnarok</s>',\n",
       " \"<s>[INST] In this task, you are asked to summarize a text. The input is one paragraph containing up to 300 words. Your job is to return the top 5 most important sentences from the given text as a list in order of importance.\\nI have three sisters and two brothers. We are all close in age, so we are very close growing up. My favorite sister is the middle one. She was always so kind and caring, even when we were fighting over who got which toy growing up. Now that we are older, she's still my best friend even though we live in different states. I talk to her every day and visit her whenever I can.\\nThe output should be a list of strings consisting of the 5 most important sentences from the text in order of importance with each string being no more than 20 words long.. [/INST] ['My favorite sister is the middle one.',\\n 'She was always so kind and caring, even when we were fighting over who got which toy growing up.',\\n 'Now that we are older, she's still my best friend even though we live in different states.',\\n 'I talk to her every day and visit her whenever I can.']</s>\"]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = dataset_dict['test'][0:2]['instances']\n",
    "[get_prompt(per_inst_d_li) for per_inst_d_li in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# ft_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 ['Ant-Man\\nCaptain America: Civil War\\nGuardians of the Galaxy Vol. 2\\nLogan\\nSpider-Man: Homecoming\\nThor Ragnarok', \"['My favorite sister is the middle one.',\\n 'She was always so kind and caring, even when we were fighting over who got which toy growing up.',\\n 'Now that we are older, she's still my best friend even though we live in different states.',\\n 'I talk to her every day and visit her whenever I can.']\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[INST] You are given a list of movie ratings. ...</td>\n",
       "      <td>[INST] You are given a list of movie ratings. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST] In this task, you are asked to summariz...</td>\n",
       "      <td>[INST] In this task, you are asked to summariz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[INST] In this task, you are given a natural l...</td>\n",
       "      <td>[INST] In this task, you are given a natural l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[INST] In this task, you are given an incomple...</td>\n",
       "      <td>[INST] In this task, you are given an incomple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[INST] Given a letter and an encrypted message...</td>\n",
       "      <td>[INST] Given a letter and an encrypted message...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[INST] It is common for people to use social n...</td>\n",
       "      <td>[INST] It is common for people to use social n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[INST] In this task, you are given three image...</td>\n",
       "      <td>[INST] In this task, you are given three image...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[INST] You are provided with a description of ...</td>\n",
       "      <td>[INST] You are provided with a description of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[INST] In this task, you are given a list of s...</td>\n",
       "      <td>[INST] In this task, you are given a list of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[INST] In this task, you will be given a piece...</td>\n",
       "      <td>[INST] In this task, you will be given a piece...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            original_model_summaries  \\\n",
       "0  [INST] You are given a list of movie ratings. ...   \n",
       "1  [INST] In this task, you are asked to summariz...   \n",
       "2  [INST] In this task, you are given a natural l...   \n",
       "3  [INST] In this task, you are given an incomple...   \n",
       "4  [INST] Given a letter and an encrypted message...   \n",
       "5  [INST] It is common for people to use social n...   \n",
       "6  [INST] In this task, you are given three image...   \n",
       "7  [INST] You are provided with a description of ...   \n",
       "8  [INST] In this task, you are given a list of s...   \n",
       "9  [INST] In this task, you will be given a piece...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  [INST] You are given a list of movie ratings. ...  \n",
       "1  [INST] In this task, you are asked to summariz...  \n",
       "2  [INST] In this task, you are given a natural l...  \n",
       "3  [INST] In this task, you are given an incomple...  \n",
       "4  [INST] Given a letter and an encrypted message...  \n",
       "5  [INST] It is common for people to use social n...  \n",
       "6  [INST] In this task, you are given three image...  \n",
       "7  [INST] You are provided with a description of ...  \n",
       "8  [INST] In this task, you are given a list of s...  \n",
       "9  [INST] In this task, you will be given a piece...  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_instances = dataset_dict['test'][0:10]['instances']\n",
    "prompts = [get_prompt(per_inst_d_li) for per_inst_d_li in eval_instances]\n",
    "outputs = [get_output(per_inst_d_li) for per_inst_d_li in eval_instances]\n",
    "#print(prompts)\n",
    "print(len(prompts), len(outputs), outputs[0:2])\n",
    "\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "#     prompt = f\"\"\"\n",
    "# Summarize the following conversation.\n",
    "\n",
    "# {dialogue}\n",
    "\n",
    "# Summary: \"\"\"\n",
    "    \n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    original_model_outputs = original_model.generate(**model_input, max_new_tokens=100)\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(**model_input, max_new_tokens=100)\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(original_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['original_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] You are given a list of movie ratings. Each rating consists of a movie title and the corresponding rating out of 5 stars. The task is to find all movies with a rating greater than or equal to 4 stars and print them in alphabetical order.\n",
      "Ant-Man - 4stars Captain America: Civil War - 5stars Deadpool - 3stars Guardians of the Galaxy Vol. 2 - 5stars Logan - 4stars Spider-Man: Homecoming - 5stars Thor Ragnarok - 4stars.\n",
      "Print each movie title on a new line in alphabetical order. [/INST] Ant-Man\n",
      "Captain America: Civil War\n",
      "Guardians of the Galaxy Vol. 2\n",
      "Logan\n",
      "Spider-Man: Homecoming\n",
      "Thor Ragnarok\n"
     ]
    }
   ],
   "source": [
    "print(df['original_model_summaries'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] You are given a list of movie ratings. Each rating consists of a movie title and the corresponding rating out of 5 stars. The task is to find all movies with a rating greater than or equal to 4 stars and print them in alphabetical order.\n",
      "Ant-Man - 4stars Captain America: Civil War - 5stars Deadpool - 3stars Guardians of the Galaxy Vol. 2 - 5stars Logan - 4stars Spider-Man: Homecoming - 5stars Thor Ragnarok - 4stars.\n",
      "Print each movie title on a new line in alphabetical order. [/INST] Ant-Man\n",
      "Captain America: Civil War\n",
      "Guardians of the Galaxy Vol. 2\n",
      "Logan\n",
      "Spider-Man: Homecoming\n",
      "Thor Ragnarok\n"
     ]
    }
   ],
   "source": [
    "print(df['peft_model_summaries'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral_ai_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
