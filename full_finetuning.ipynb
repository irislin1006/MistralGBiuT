{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Fine-Tune Mistral 7B using LoRA\n",
    "We use [unnatural-instructions](https://github.com/orhonovich/unnatural-instructions) to perform instruction fune-tuning on mistral 7B base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.59s/it]\n"
     ]
    }
   ],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config) # .to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512, padding_side=\"left\", add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print number of trainable model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 262410240\n",
      "all model parameters: 3752071168\n",
      "percentage of trainable model parameters: 6.99%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Unnatural Instruction Dataset through Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 66010\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"mrm8488/unnatural-instructions-core\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explode 'instances' into separate rows and make sure exploded_dataset remains the same schema as the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 68478\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_datasets = {}\n",
    "\n",
    "for split, ds in dataset.items():\n",
    "    # Prepare lists to hold the exploded rows\n",
    "    exploded_instructions, exploded_instances = [], []\n",
    "\n",
    "    # Iterate over each row in the dataset\n",
    "    for row in ds:\n",
    "        instruction = row['instruction']\n",
    "        for instance in row['instances']:\n",
    "            # For each instance, create a new row with the same instruction\n",
    "            exploded_instructions.append(instruction)\n",
    "            exploded_instances.append([instance])\n",
    "    \n",
    "    # Create a new dataset from the exploded rows\n",
    "    exploded_data = {'instruction': exploded_instructions, 'instances': exploded_instances}\n",
    "    new_datasets[split] = Dataset.from_dict(exploded_data)\n",
    "\n",
    "# Combine the new datasets into a DatasetDict\n",
    "exploded_dataset = DatasetDict(new_datasets)\n",
    "\n",
    "exploded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'You will be given a series of words. Output these words in reverse order, with each word on its own line.',\n",
       " 'instances': [{'constraints': 'None.',\n",
       "   'input': \"Words: ['Hello', 'world'].\",\n",
       "   'instruction_with_input': \"You will be given a series of words. Output these words in reverse order, with each word on its own line.\\nWords: ['Hello', 'world'].\",\n",
       "   'output': 'world\\nHello'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check each instance format\n",
    "exploded_dataset['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to make sure that each row has only one instance\n",
    "sanity_df = exploded_dataset['train'].to_pandas()\n",
    "\n",
    "sanity_df['instances_length'] = sanity_df['instances'].apply(len)\n",
    "assert sanity_df[sanity_df['instances_length'] != 1].empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train, val, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 54782\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 6848\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['instruction', 'instances'],\n",
       "        num_rows: 6848\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into train, test, and validation sets\n",
    "train_test_dataset = exploded_dataset['train'].train_test_split(test_size=0.2, seed=42)  # 80% train, 20% for test and validation\n",
    "test_val_dataset = train_test_dataset['test'].train_test_split(test_size=0.5, seed=42)  # Split the 20% equally into test and validation\n",
    "\n",
    "# Create a new DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_test_dataset['train'],\n",
    "    'test': test_val_dataset['train'],\n",
    "    'validation': test_val_dataset['test']\n",
    "})\n",
    "\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instructions: You are given a passage with certain words/phrases bolded. Identify if the word/phrase is being used in context ofSTAGE DIRECTIONS orCHARACTERIZATION.\n",
      "instances: [{'constraints': 'The output for each word should be either 0 (Stage Direction) or 1 (Characterization).', 'input': 'STAGE DIRECTIONS:As he enters,MACBETH sees the three witches.He stopshis steps CHARACTERIZATION: amazed and terrified at their sight.', 'instruction_with_input': 'You are given a passage with certain words/phrases bolded. Identify if the word/phrase is being used in context ofSTAGE DIRECTIONS orCHARACTERIZATION.\\nSTAGE DIRECTIONS:As he enters,MACBETH sees the three witches.He stopshis steps CHARACTERIZATION: amazed and terrified at their sight.', 'output': '0\\n1\\n0\\n1'}]\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "print(f\"instructions: {dataset_dict['test'][index]['instruction']}\")\n",
    "print(f\"instances: {dataset_dict['test'][index]['instances']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Dataset to Generate Prompt|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 54782/54782 [00:07<00:00, 6990.22 examples/s]\n",
      "Map: 100%|██████████| 6848/6848 [00:01<00:00, 6547.38 examples/s]\n",
      "Map: 100%|██████████| 6848/6848 [00:00<00:00, 7508.51 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 54782\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 6848\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 6848\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prompt(d_li):\n",
    "    p_li  = []\n",
    "    for d in d_li:\n",
    "        constraints = f\"\\n{d['constraints']}\" if d['constraints'] else \"\"\n",
    "        instruction = f\"{d['instruction_with_input']}{constraints}\"\n",
    "        model_answer = d['output']\n",
    "\n",
    "        prompt = f\"<s>[INST] {instruction} [/INST] {model_answer}</s>\"\n",
    "        p_li.append(prompt)\n",
    "    return \"\\n\".join(p_li)\n",
    "\n",
    "def get_output(d_li):\n",
    "    return \"\\n\".join([d['output'] for d in d_li])\n",
    "\n",
    "def tokenize_function(example):\n",
    "    prompts = [get_prompt(per_inst_d_li) for per_inst_d_li in example['instances']]\n",
    "    tokenized_example = tokenizer(prompts, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    example['input_ids'] = tokenized_example.input_ids # .to(device)\n",
    "    example['labels'] = tokenized_example.input_ids.clone()\n",
    "    return example\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True, remove_columns=['instruction', 'instances']) # \n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 54782/54782 [00:10<00:00, 5466.24 examples/s]\n",
      "Filter: 100%|██████████| 6848/6848 [00:01<00:00, 5466.00 examples/s]\n",
      "Filter: 100%|██████████| 6848/6848 [00:01<00:00, 5465.93 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 548\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 69\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 69\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To save some time in the lab, you will subsample the dataset:\n",
    "subsampled_tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "\n",
    "subsampled_tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (54782, 2)\n",
      "Validation: (6848, 2)\n",
      "Test: (6848, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 54782\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 6848\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 6848\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input len is 512\n",
      "input Ids: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 733, 16289, 28793, 995, 622, 347, 2078, 264, 5511, 302, 2245, 2477, 684, 396, 15048, 1951, 28725, 442, 264, 2952, 6251, 28723, 1047, 272, 1951, 3969, 8313, 354, 368, 28725, 442, 272, 2952, 6251, 2368, 28742, 28707, 1038, 3367, 298, 2918, 574, 901, 1053, 1058, 28725, 3825, 464, 2012, 7285, 647, 5860, 3825, 464, 2249, 1899, 1070, 4135, 13, 1874, 28747, 415, 5252, 5970, 302, 264, 1712, 28733, 18387, 3667, 304, 1539, 28742, 28707, 1721, 778, 7769, 28723, 13, 1014, 3825, 1023, 347, 624, 302, 272, 989, 28747, 464, 2012, 7285, 28742, 442, 464, 2249, 1899, 1070, 4135, 733, 28748, 16289, 28793, 2236, 7285, 2, 2]\n",
      "labels: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 733, 16289, 28793, 995, 622, 347, 2078, 264, 5511, 302, 2245, 2477, 684, 396, 15048, 1951, 28725, 442, 264, 2952, 6251, 28723, 1047, 272, 1951, 3969, 8313, 354, 368, 28725, 442, 272, 2952, 6251, 2368, 28742, 28707, 1038, 3367, 298, 2918, 574, 901, 1053, 1058, 28725, 3825, 464, 2012, 7285, 647, 5860, 3825, 464, 2249, 1899, 1070, 4135, 13, 1874, 28747, 415, 5252, 5970, 302, 264, 1712, 28733, 18387, 3667, 304, 1539, 28742, 28707, 1721, 778, 7769, 28723, 13, 1014, 3825, 1023, 347, 624, 302, 272, 989, 28747, 464, 2012, 7285, 28742, 442, 464, 2249, 1899, 1070, 4135, 733, 28748, 16289, 28793, 2236, 7285, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"input len is {len(tokenized_datasets['train'][4]['input_ids'])}\")\n",
    "print(f\"input Ids: {tokenized_datasets['train'][0]['input_ids']}\")\n",
    "print(f\"labels: {tokenized_datasets['train'][0]['labels']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fin-Tune the Model with Preprocessed Dataset\n",
    "(Warning) It will OOM on my machine, so I marked them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = f'./model/dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     learning_rate=1e-5,\n",
    "#     per_device_train_batch_size=2,\n",
    "#     num_train_epochs=1,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_steps=1,\n",
    "#     max_steps=1,\n",
    "#     fp16=True,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=original_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets['train'],\n",
    "#     eval_dataset=tokenized_datasets['validation'],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train() ### OOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 3752071168\n",
      "percentage of trainable model parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "original_model.enable_input_require_grads()\n",
    "original_model.gradient_checkpointing_enable()\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type='CAUSAL_LM'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 21260288\n",
      "all model parameters: 3773331456\n",
      "percentage of trainable model parameters: 0.56%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "run_name = f'peft-causal-lm-training-{str(int(time.time()))}'\n",
    "output_dir = f'./' + run_name\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    warmup_steps=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
    "    logging_steps=50,\n",
    "    bf16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_dir=\"./logs\",        # Directory for storing logs\n",
    "    save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "    save_steps=50,                # Save checkpoints every 50 steps\n",
    "    evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "    eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "    do_eval=True,                # Perform evaluation at the end of training\n",
    "    report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "    run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mirislin1006\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/iris/Desktop/wsl_shared/mistralAi/wandb/run-20240126_102548-wafzyl96</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/irislin1006/huggingface/runs/wafzyl96' target=\"_blank\">peft-causal-lm-training-1706286347-2024-01-26-10-25</a></strong> to <a href='https://wandb.ai/irislin1006/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/irislin1006/huggingface' target=\"_blank\">https://wandb.ai/irislin1006/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/irislin1006/huggingface/runs/wafzyl96' target=\"_blank\">https://wandb.ai/irislin1006/huggingface/runs/wafzyl96</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 9:11:36, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.274000</td>\n",
       "      <td>1.211107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.118400</td>\n",
       "      <td>1.104863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.077300</td>\n",
       "      <td>1.064886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.037300</td>\n",
       "      <td>1.037202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.021100</td>\n",
       "      <td>1.020360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.012600</td>\n",
       "      <td>1.006043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.991200</td>\n",
       "      <td>0.989309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.963900</td>\n",
       "      <td>0.980859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.954400</td>\n",
       "      <td>0.973180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.973800</td>\n",
       "      <td>0.965228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.970900</td>\n",
       "      <td>0.959576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.946500</td>\n",
       "      <td>0.956385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.935700</td>\n",
       "      <td>0.950263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.936900</td>\n",
       "      <td>0.945787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.946300</td>\n",
       "      <td>0.942487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.912200</td>\n",
       "      <td>0.939733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.919400</td>\n",
       "      <td>0.937060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.913000</td>\n",
       "      <td>0.935345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.924500</td>\n",
       "      <td>0.934340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>0.933573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.0370360374450684, metrics={'train_runtime': 33107.1791, 'train_samples_per_second': 0.242, 'train_steps_per_second': 0.03, 'total_flos': 1.75274075357184e+17, 'train_loss': 1.0370360374450684, 'epoch': 0.15})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/peft/utils/save_and_load.py:131: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n",
       " './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n",
       " './peft-dialogue-summary-checkpoint-local/tokenizer.model',\n",
       " './peft-dialogue-summary-checkpoint-local/added_tokens.json',\n",
       " './peft-dialogue-summary-checkpoint-local/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>[INST] You are given a list of movie ratings. Each rating consists of a movie title and the corresponding rating out of 5 stars. The task is to find all movies with a rating greater than or equal to 4 stars and print them in alphabetical order.\\nAnt-Man - 4stars Captain America: Civil War - 5stars Deadpool - 3stars Guardians of the Galaxy Vol. 2 - 5stars Logan - 4stars Spider-Man: Homecoming - 5stars Thor Ragnarok - 4stars.\\nPrint each movie title on a new line in alphabetical order. [/INST] Ant-Man\\nCaptain America: Civil War\\nGuardians of the Galaxy Vol. 2\\nLogan\\nSpider-Man: Homecoming\\nThor Ragnarok</s>',\n",
       " \"<s>[INST] In this task, you are asked to summarize a text. The input is one paragraph containing up to 300 words. Your job is to return the top 5 most important sentences from the given text as a list in order of importance.\\nI have three sisters and two brothers. We are all close in age, so we are very close growing up. My favorite sister is the middle one. She was always so kind and caring, even when we were fighting over who got which toy growing up. Now that we are older, she's still my best friend even though we live in different states. I talk to her every day and visit her whenever I can.\\nThe output should be a list of strings consisting of the 5 most important sentences from the text in order of importance with each string being no more than 20 words long.. [/INST] ['My favorite sister is the middle one.',\\n 'She was always so kind and caring, even when we were fighting over who got which toy growing up.',\\n 'Now that we are older, she's still my best friend even though we live in different states.',\\n 'I talk to her every day and visit her whenever I can.']</s>\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = dataset_dict['test'][0:2]['instances']\n",
    "[get_prompt(per_inst_d_li) for per_inst_d_li in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# ft_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 ['Ant-Man\\nCaptain America: Civil War\\nGuardians of the Galaxy Vol. 2\\nLogan\\nSpider-Man: Homecoming\\nThor Ragnarok', \"['My favorite sister is the middle one.',\\n 'She was always so kind and caring, even when we were fighting over who got which toy growing up.',\\n 'Now that we are older, she's still my best friend even though we live in different states.',\\n 'I talk to her every day and visit her whenever I can.']\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iris/anaconda3/envs/mistral_ai_gpt/lib/python3.9/site-packages/transformers/generation/utils.py:1636: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_model_generation</th>\n",
       "      <th>peft_model_generation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[INST] You are given a list of movie ratings. ...</td>\n",
       "      <td>[INST] You are given a list of movie ratings. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[INST] In this task, you are asked to summariz...</td>\n",
       "      <td>[INST] In this task, you are asked to summariz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[INST] In this task, you are given a natural l...</td>\n",
       "      <td>[INST] In this task, you are given a natural l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[INST] In this task, you are given an incomple...</td>\n",
       "      <td>[INST] In this task, you are given an incomple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[INST] Given a letter and an encrypted message...</td>\n",
       "      <td>[INST] Given a letter and an encrypted message...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[INST] It is common for people to use social n...</td>\n",
       "      <td>[INST] It is common for people to use social n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[INST] In this task, you are given three image...</td>\n",
       "      <td>[INST] In this task, you are given three image...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[INST] You are provided with a description of ...</td>\n",
       "      <td>[INST] You are provided with a description of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[INST] In this task, you are given a list of s...</td>\n",
       "      <td>[INST] In this task, you are given a list of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[INST] In this task, you will be given a piece...</td>\n",
       "      <td>[INST] In this task, you will be given a piece...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           original_model_generation  \\\n",
       "0  [INST] You are given a list of movie ratings. ...   \n",
       "1  [INST] In this task, you are asked to summariz...   \n",
       "2  [INST] In this task, you are given a natural l...   \n",
       "3  [INST] In this task, you are given an incomple...   \n",
       "4  [INST] Given a letter and an encrypted message...   \n",
       "5  [INST] It is common for people to use social n...   \n",
       "6  [INST] In this task, you are given three image...   \n",
       "7  [INST] You are provided with a description of ...   \n",
       "8  [INST] In this task, you are given a list of s...   \n",
       "9  [INST] In this task, you will be given a piece...   \n",
       "\n",
       "                               peft_model_generation  \n",
       "0  [INST] You are given a list of movie ratings. ...  \n",
       "1  [INST] In this task, you are asked to summariz...  \n",
       "2  [INST] In this task, you are given a natural l...  \n",
       "3  [INST] In this task, you are given an incomple...  \n",
       "4  [INST] Given a letter and an encrypted message...  \n",
       "5  [INST] It is common for people to use social n...  \n",
       "6  [INST] In this task, you are given three image...  \n",
       "7  [INST] You are provided with a description of ...  \n",
       "8  [INST] In this task, you are given a list of s...  \n",
       "9  [INST] In this task, you will be given a piece...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_instances = dataset_dict['test'][0:10]['instances']\n",
    "prompts = [get_prompt(per_inst_d_li) for per_inst_d_li in eval_instances]\n",
    "outputs = [get_output(per_inst_d_li) for per_inst_d_li in eval_instances]\n",
    "#print(prompts)\n",
    "print(len(prompts), len(outputs), outputs[0:2])\n",
    "\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    model_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    original_model_outputs = original_model.generate(**model_input, max_new_tokens=100)\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(**model_input, max_new_tokens=100)\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(original_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['original_model_generation', 'peft_model_generation'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] You are given a list of movie ratings. Each rating consists of a movie title and the corresponding rating out of 5 stars. The task is to find all movies with a rating greater than or equal to 4 stars and print them in alphabetical order.\n",
      "Ant-Man - 4stars Captain America: Civil War - 5stars Deadpool - 3stars Guardians of the Galaxy Vol. 2 - 5stars Logan - 4stars Spider-Man: Homecoming - 5stars Thor Ragnarok - 4stars.\n",
      "Print each movie title on a new line in alphabetical order. [/INST] Ant-Man\n",
      "Captain America: Civil War\n",
      "Guardians of the Galaxy Vol. 2\n",
      "Logan\n",
      "Spider-Man: Homecoming\n",
      "Thor Ragnarok Question 2\n",
      "\n",
      "You are given a list of movie ratings. Each rating consists of a movie title and the corresponding rating out of 5 stars. The task is to find all movies with a rating greater than or equal to 4 stars and print them in alphabetical order.\n",
      "\n",
      "Ant-Man - 4stars Captain America: Civil War - 5stars Deadpool - 3stars Guardians of the Galaxy Vol. 2 - 5stars\n"
     ]
    }
   ],
   "source": [
    "print(df['original_model_generation'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] You are given a list of movie ratings. Each rating consists of a movie title and the corresponding rating out of 5 stars. The task is to find all movies with a rating greater than or equal to 4 stars and print them in alphabetical order.\n",
      "Ant-Man - 4stars Captain America: Civil War - 5stars Deadpool - 3stars Guardians of the Galaxy Vol. 2 - 5stars Logan - 4stars Spider-Man: Homecoming - 5stars Thor Ragnarok - 4stars.\n",
      "Print each movie title on a new line in alphabetical order. [/INST] Ant-Man\n",
      "Captain America: Civil War\n",
      "Guardians of the Galaxy Vol. 2\n",
      "Logan\n",
      "Spider-Man: Homecoming\n",
      "Thor Ragnarok Question 2\n",
      "\n",
      "You are given a list of movie ratings. Each rating consists of a movie title and the corresponding rating out of 5 stars. The task is to find all movies with a rating greater than or equal to 4 stars and print them in alphabetical order.\n",
      "\n",
      "Ant-Man - 4stars Captain America: Civil War - 5stars Deadpool - 3stars Guardians of the Galaxy Vol. 2 - 5stars\n"
     ]
    }
   ],
   "source": [
    "print(df['peft_model_generation'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model result:\n",
      "[INST] It is common for people to use social networking sites, such as Facebook and Twitter, to communicate with each other. In this task you are given two tweets from different users. The job is to find out whether the two tweets are in response to each other or not. We will mark the tweet pair as 'True' if it is in response to each other, otherwise 'False'.\n",
      "@jsbyun86 hello! how are you? @ohyeskim fine thanks! how about you?\n",
      "The output should be 'True' or 'False'. [/INST] True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "Peft model result:\n",
      "[INST] It is common for people to use social networking sites, such as Facebook and Twitter, to communicate with each other. In this task you are given two tweets from different users. The job is to find out whether the two tweets are in response to each other or not. We will mark the tweet pair as 'True' if it is in response to each other, otherwise 'False'.\n",
      "@jsbyun86 hello! how are you? @ohyeskim fine thanks! how about you?\n",
      "The output should be 'True' or 'False'. [/INST] True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n",
      "True\n",
      "\n",
      "False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original model result:\")\n",
    "print(df['original_model_generation'].iloc[5])\n",
    "print(\"Peft model result:\")\n",
    "print(df['peft_model_generation'].iloc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model result:\n",
      "[INST] In this task, you are given an incomplete sentence with one or more missing words. Your job is to predict the most probable word(s) that can complete the sentence based on common sense and reasoning.\n",
      "I never _ a chance to meet her.\n",
      "The output should be one or more words that can complete the given sentence. [/INST] I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original model result:\")\n",
    "print(df['original_model_generation'].iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model result:\n",
      "[INST] In this task, you are given an incomplete sentence with one or more missing words. Your job is to predict the most probable word(s) that can complete the sentence based on common sense and reasoning.\n",
      "I never _ a chance to meet her.\n",
      "The output should be one or more words that can complete the given sentence. [/INST] I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n",
      "I never had a chance to meet her.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Peft model result:\")\n",
    "print(df['peft_model_generation'].iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral_ai_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
